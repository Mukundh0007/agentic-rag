# ğŸ” Visual-First Financial Document Intelligence Agent

> **Multimodal RAG system that uses Computer Vision + LLMs to extract and query financial data from complex PDFs**

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.52-FF4B4B.svg)](https://streamlit.io)
[![LlamaIndex](https://img.shields.io/badge/LlamaIndex-0.12-000000.svg)](https://llamaindex.ai)
[![OpenRouter](https://img.shields.io/badge/OpenRouter-AI-purple.svg)](https://openrouter.ai)

---

## ğŸ¯ Problem Statement

Financial analysts spend **hours** manually cross-referencing data between narrative text and tables in documents like 10-K filings. Traditional OCR solutions fail because they:

- Treat documents as plain text (losing table structure)
- Can't handle complex layouts with charts and multi-column formats
- Don't understand financial context or "read" tables visually

## ğŸ’¡ Solution

A **Vision-First RAG Pipeline** that:

1. **Ingests** financial PDFs (10-K filings).
2. **Extracts** structured data using **GPT-4o-mini** (via OpenRouter) and strict PDF parsing.
3. **Summarizes** tables visually using Vision-Language Models.
4. **Indexes** visual and textual content into a local vector database.
5. **Answers** natural language queries and **shows the actual source table image** for verification.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF File  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingestion Engine   â”‚
â”‚  (src/rag/ingest)   â”‚
â”‚  â€¢ PDF Parsing      â”‚
â”‚  â€¢ Vision Summarizerâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Text Chunks + Table Summaries
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Database    â”‚  â† LlamaIndex + Embeddings
â”‚  (Local Storage)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Engine       â”‚ â—„â”€â”€â”€â”€ â–º  OpenRouter LLM â”‚
â”‚  (src/rag/query)    â”‚       â”‚ (GPT-4o/Gemini) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chat Interface     â”‚  â† Streamlit UI
â”‚  (app.py)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.12+
- `uv` (recommended) or `pip`
- OpenRouter API Key

### 2. Installation

```bash
git clone https://github.com/Mukundh0007/agentic-rag.git
cd agentic-rag

# Install dependencies with uv (fastest)
uv sync

# Or with pip
pip install -r requirements.txt
```

### 3. Configuration

Create a `.env` file in the root directory:

```env
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxx...
```

### 4. Usage

We provide a central controller `main.py` for most tasks, but the computer vision pipeline requires initialization.

**Step 1: Setup Models**
Download YOLOv8 weights and the fine-tuned table detector.

```bash
uv run python src/download_weights.py
```

**Step 2: Extract Tables (Computer Vision)**
To verify the environment, run:

```bash
uv run python src/verify.py
```

Then, run the YOLOv8 pipeline to crop tables from your PDF.
```bash
uv run python src/vision/vision_processor.py
```

*(This saves images to `data/processed_tables/`)*

**Step 3: Ingest Data**
Process the PDF text and the extracted table images into the vector index.

```bash
uv run python main.py --ingest
```

**Step B: Launch Web App (The "Wow" Factor)**
Start the visual chat interface.

```bash
uv run python main.py --app
```

**Step C: CLI Query (Optional)**
Run a quick test query from the terminal.

```bash
uv run python main.py --query "What are the primary risk factors?"
```

---

## ğŸ“‚ Project Structure

```
agentic-rag/
â”œâ”€â”€ ğŸ“„ main.py                      # ğŸ® Central CLI controller
â”œâ”€â”€ ğŸ“„ app.py                       # ğŸ–¥ï¸ Streamlit Web Application
â”œâ”€â”€ ğŸ“„ pyproject.toml               # Dependency configuration
â”œâ”€â”€ ğŸ“„ requirements.txt             # Pip requirements
â”œâ”€â”€ ğŸ“„ README.md                    # Documentation
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â””â”€â”€ ğŸ“‚ rag/
â”‚       â”œâ”€â”€ ğŸ“„ ingest.py            # ğŸ—ï¸ Ingestion pipeline (PDF -> Vector DB)
â”‚       â”œâ”€â”€ ğŸ“„ query.py             # ğŸ” Retrieval & Query logic
â”‚       â””â”€â”€ ğŸ“„ openrouter_client.py # ğŸ”Œ Custom LlamaIndex adapter for OpenRouter
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ ğŸ“„ apple_10k.pdf            # Source Document
â”‚   â””â”€â”€ ğŸ“‚ processed_tables/        # Extracted table images
â”‚
â””â”€â”€ ğŸ“‚ storage/                     # Local Vector Store (created after ingest)
```

---

## âœ¨ Key Features

- **Robust PDF Parsing**: Uses `PDFReader` for accurate text extraction (no garbage binary text).
- **Visual Verification**: The chatbot displays the **actual images** of the tables it used to answer your question.
- **Smart Routing**: `main.py` handles CLI commands and app launching seamlessly.
- **Cost Effective**: Optimized to use efficient models like `gpt-4o-mini` via OpenRouter.

---

## ğŸ“Š Example Interaction

**User Query**: *"What was the total net sales in 2024?"*

**AI Response**:
> Apple's total net sales in 2024 were **$391.04 billion**.

**Verified Sources**:

- `p23_table_5.png` (Shows the Income Statement)
- `p32_table_13.png` (Shows Segment Breakdown)

*(The UI displays these images automatically)*

---

## ğŸ“ License

MIT License
